---
layout: cogsci-card
title: 'Bayes'
references:
    - 
---

Bayes Theorem is a crucial tool for probability theory. Bayes Theorem is a specific application of **conditional probability**. Conditional probability is a method of measuring the influence of one event on another. In mathematical notation, P(A|B) is read "probability of event A given that event B has occurred" and means similarly. The probability of $$A$$ occurring might change is we know that B has happened, as B might influence the conditions of A transpiring. You can view conditional probability as introducing more information about the context of the event. For example, the probability of a horse winning a race might be 0.2 (20%), but the probability of that same horse winning the race GIVEN the horse was mildly sick very recently would be lower. The probability might also be reduced GIVEN the horse has never won in its 20 race track-record. Even though this information provides nothing directly about the physical horse, this extra information influences your own subjective probability of the horse winning this race. Conditional probability is calculated as:

With some clever algebra, one can derive Bayes Theorem. Bayes Theorem is another method for computing conditional probabilities. In particular, Bayes theorem is useful for updating existing information based on new data. As more information is obtained, better inferences about the subject at hand can be made.

Bayes is increasingly common in cognitive sciences, particularly in neuroscience and computer science. More work has been done with probabilistic models in computer science as a basis for machine learning, where inference is conducted on model parameters based on provided data.